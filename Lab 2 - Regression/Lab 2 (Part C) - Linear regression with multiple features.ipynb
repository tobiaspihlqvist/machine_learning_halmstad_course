{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "delitable": false
   },
   "source": [
    "# Lab 2 (Part C) - Linear regression with multiple features\n",
    "\n",
    "Make sure that you check the videos of lecture 2 before starting this Lab:\n",
    "- Introduction to Linear Regression: https://youtu.be/-wmjwMWRsZU\n",
    "- Introduction to Nonlinear Regression: https://youtu.be/Hyu8QMLEHrE\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Please complete this Jupyter Notebook and upload it as a zip file to Blackboard, preferably __before 21 Sept 2020__. You can ask questions and get help in the \"Discussions\" section on Blackboard, or during the live Monday sessions in Zoom.\n",
    "</div>\n",
    "\n",
    "In this part of the lab, you will implement linear regression with multiple variables to predict the price of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "# 1. Loading the dataset\n",
    "The file `housing-dataset.csv` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. The following Python code helps you load the dataset from the data file into the variables $X$ and $y$. Read the code and print a small subset of $X$ and $y$ to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "[[2104.    3.]\n",
      " [1600.    3.]\n",
      " [2400.    3.]]\n",
      "[399900. 329900.]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/housing-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "\n",
    "# We have n data-points (houses)\n",
    "n = len(mydata)\n",
    "\n",
    "# X is a matrix of two column, i.e. an array of n 2-dimensional data-points\n",
    "X = mydata[:, :2].reshape(n, 2)\n",
    "\n",
    "# y is the vector of outputs, i.e. an array of n scalar values\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can print a small subset of X and y to see what it looks like.\n",
    "\"\"\"\n",
    "print(n);\n",
    "print(X[:3])\n",
    "print(y[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data normalization (scaling or standardization)\n",
    "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. Your task here is to write the following code to:\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective *standard deviations*.\n",
    "\n",
    "In Python, you can use the numpy function `np.mean(..)` to compute the mean. This function can directly be used on a $d$-dimensional dataset to compute a $d$-dimensional mean vector `mu` where each value `mu[j]` is the mean of the $j^{th}$ feature. This is done by setting the $2^{nd}$ argument `axis` of this function to `0`. For example, consider the following matrix `A` where each line corresponds to one data-point and each column corresponds to one feature:\n",
    "\n",
    "```python\n",
    "A = [[ 100,    10],\n",
    "     [ 30,     10],\n",
    "     [ 230,    25]]\n",
    "```\n",
    "\n",
    "In this case, `np.mean(A, axis=0)` will give `[120,   15]` where 120 is the mean of the 1st  column (1st feature) and 15 is the mean of the 2nd column (2nd feature). Another function `np.std(..)` exists to compute the standard deviation. The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (usually, most data points will lie within the interval: mean $\\pm$ 2 standard_deviation).\n",
    "\n",
    "Once the features are normalized, you can do a scatter plot of the original dataset `X` (size of the house vs. number of bedrooms) and a scatter plot of the normalized dataset `X_normalized`. You will notice that the normalized dataset still have the same shape as the original one; the difference is that the new feature values have a similar scale and are centred arround the origin.\n",
    "\n",
    "**Implementation Note**: When normalizing the features, it is important to store the values used for normalization (the mean and the standard deviation used for the computations). Indeed, after learning the parameters of a model, we often want to predict the prices of houses we have not seen before. Given a new $x$ value (living room area and number of bedrooms), we must first normalize $x$ using the mean and standard deviation that we had previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the following code to compute a normalized version of X called: X_normalized\n",
    "\"\"\"\n",
    "# TODO: compute mu, the mean vector from X\n",
    "# TODO: compute std, the standard deviation vector from X\n",
    "mu = np.mean(X, 0);\n",
    "std = np.std(X);\n",
    "X_normalized = (X - mu) / std\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "- Do a scatter plot of the original dataset X\n",
    "- Do a scatter plot of the normalized dataset X_normalized\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "# ax.scatter(X[:, 0], X[:, 1], color=\"red\",marker=\"x\")\n",
    "ax.scatter(X_normalized[:, 0], X[:, 1], color=\"red\",marker=\"x\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what you did in Lab2 Part B, you can simplify your implementation of linear regression by adding an additional first column to `X_normalized` with all the values of this column set to $1$. To do this you can re-use the function `add_all_ones_column(..)` defined in Lab2 Part B, which takes a matrix as argument and returns a new matrix with an additional first column (of ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of X_normalized_new\n",
      "[[ 1.00000000e+00  9.03887792e-02 -1.48910674e-04]\n",
      " [ 1.00000000e+00 -3.50535727e-01 -1.48910674e-04]\n",
      " [ 1.00000000e+00  3.49344441e-01 -1.48910674e-04]\n",
      " [ 1.00000000e+00 -5.11508165e-01 -1.02376088e-03]\n",
      " [ 1.00000000e+00  8.74254567e-01  7.25939536e-04]\n",
      " [ 1.00000000e+00 -1.37183958e-02  7.25939536e-04]\n",
      " [ 1.00000000e+00 -4.08275841e-01 -1.48910674e-04]\n",
      " [ 1.00000000e+00 -5.01884813e-01 -1.48910674e-04]\n",
      " [ 1.00000000e+00 -5.43002773e-01 -1.48910674e-04]\n",
      " [ 1.00000000e+00 -4.43269849e-01 -1.48910674e-04]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Copy-past here the definition of the function add_all_ones_column(...) that \n",
    "you have see in Lab 2 (Part B).\n",
    "\"\"\"\n",
    "# definition of the function add_all_ones_column() here ...\n",
    "def add_all_ones_column(X):\n",
    "    n, d = X.shape # dimension of the matrix X (n lines, d columns)\n",
    "    XX = np.ones((n, d+1)) # new matrix of all ones with one additional column\n",
    "    XX[:, 1:] = X # set X starting from column 1 (keep only column 0 unchanged)\n",
    "    return XX;\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Just uncomment the following lines to create a matrix \n",
    "X_normalized_new with an additional first column (of ones).\n",
    "\"\"\"\n",
    "X_normalized_new = add_all_ones_column(X_normalized)\n",
    "\n",
    "print(\"Subset of X_normalized_new\")\n",
    "print(X_normalized_new[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression from Scrach\n",
    "You are now ready to implement the linear regression using gradient descent (with more than one feature). In this multivariate case, you can further simply your implementation by writing the cost function in the following vectorized form:\n",
    "\n",
    "$$E(\\theta) = \\frac{1}{2n} (X \\theta - y)^T (X \\theta - y)$$\n",
    "\n",
    "$$\\text{where }\\quad\n",
    "X = \\begin{bmatrix}\n",
    "-- ~ {x^{(1)}}^T ~ -- \\\\ \n",
    "-- ~ {x^{(2)}}^T ~ -- \\\\ \n",
    "\\vdots \\\\ \n",
    "-- ~ {x^{(n)}}^T ~ --\n",
    "\\end{bmatrix}\n",
    "\\quad \\quad \\quad\n",
    "y = \\begin{bmatrix}\n",
    "y^{(1)} \\\\ \n",
    "y^{(2)} \\\\ \n",
    "\\vdots \\\\ \n",
    "y^{(n)} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The vectorized form of the gradient of $E(\\theta)$ is a vector denoted as $\\nabla E(\\theta)$ and defined follows:\n",
    "\n",
    "$$\\nabla E(\\theta) = \\left ( \\frac{\\partial E}{\\partial \\theta_0}, \\frac{\\partial E}{\\partial \\theta_1}, \\dots, \\frac{\\partial E}{\\partial \\theta_d} \\right ) = \\frac{1}{n} X^T (X \\theta - y)$$\n",
    "\n",
    "this is a **vector** where each $j^{th}$ value corresponds to $\\frac{\\partial E}{\\partial \\theta_j}$ (the derivative of the function $E$ with respect to the parameter $\\theta_j$)\n",
    "\n",
    "One your code is finished, you will get to try out different learning rates $\\alpha$ for the dataset and find a learning rate that converges quickly. To do so, you can plot the history of the cost $E(\\theta)$ with respect to the number of iterations at the end of your code.\n",
    "\n",
    "For example for alpha values of 0.01, 0.05 and 0.1, the plot should look like follows:\n",
    "<img src=\"imgs/costLab2C.png\" width=\"400px\" />\n",
    "\n",
    "If your learning rate is too large, $E(\\theta)$ can diverge and *blow up*, resulting in values which are too large for computer calculations. In these situations, Python will tend to return `NaN` or `inf` (NaN stands for \"*not a number*\" and is often caused by undefined operations that involve $-\\inf$ and $+\\inf$). If your value of $E(\\theta)$ increases or even blows up, adjust your learning rate and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.15810617e+05 5.80664464e+04 2.81615240e+01]\n",
      "[3.38397236e+05 1.39733559e+05 6.00555278e+01]\n",
      "[3.40403618e+05 1.52561351e+05 5.20632846e+01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TobiasPi\\AppData\\Local\\Temp\\ipykernel_30264\\1621331331.py:79: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Write the cost function E using the vectorized form\n",
    "\"\"\"\n",
    "def E(theta, X, y):\n",
    "    expressionOne = (X@theta -y).T;\n",
    "    expressionTwo = (X@theta-y);\n",
    "    cost = (1/2*n) * expressionOne@expressionTwo;\n",
    "    return 1/2*n * (X@theta -y).T @ (X@theta -y)\n",
    "\n",
    "\"\"\" TODO: \n",
    "Define the function grad_E (the gradient of E) using the vectorized form.\n",
    "This should return a vector of the same dimension as theta\n",
    "\"\"\"\n",
    "def grad_E(theta, X, y):\n",
    "    gradient = (1/n) * X.T @ (X @ theta -y);\n",
    "    return gradient;\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Complete the definition of the function LinearRegressionWithGD(...) below\n",
    "Note: don't forget to call the functions E(..) and grad_E(..) with X_normalized_new instead of X\n",
    "\n",
    "The arguments of LinearRegressionWithGD(..) are:\n",
    "*** theta: vector of initial parameter values\n",
    "*** alpha: the learning rate (used by gradient descent)\n",
    "*** max_iterations: maximum number of iterations to perform\n",
    "*** epsilon: to stop iterating if the cost decreases by less than epsilon\n",
    "\n",
    "The function returns:\n",
    "*** errs: a list corresponding to the historical cost values\n",
    "*** theta: the final parameter values\n",
    "\"\"\"\n",
    "def LinearRegressionWithGD(theta, alpha, max_iterations, epsilon):\n",
    "    errs = []\n",
    "    iterations = []\n",
    "    for itr in range(max_iterations):\n",
    "        iterations.append(itr)\n",
    "        mse = E(theta, X_normalized_new, y) # mean squared error\n",
    "        errs.append(mse)\n",
    "        \n",
    "        gradient = grad_E(theta, X_normalized_new, y); # derived mean squared error\n",
    "        theta = theta - alpha*gradient;\n",
    "        # TODO: take a gradient descent step to adapt the vector of parameters theta\n",
    "        # ...\n",
    "        convergence = abs(E(theta, X_normalized_new, y) - mse);\n",
    "        # print(convergence);\n",
    "        # TODO: test if the cost decreases by less than epsilon (to stop iterating)\n",
    "        if convergence < epsilon:\n",
    "            print(\"im met at itr: \", itr)\n",
    "            break;\n",
    "    \n",
    "    return errs, theta, iterations\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Here you will call LinearRegressionWithGD(..) in a loop with different values of alpha, \n",
    "and plot the cost history (errs) returned by each call of LinearRegressionWithGD(..)\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Number of Iterations\")\n",
    "ax.set_ylabel(r\"Cost $E(\\theta)$\")\n",
    "\n",
    "theta_init = np.array([0, 0, 0])\n",
    "max_iterations = 100\n",
    "epsilon = 0.000000000001\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    pass\n",
    "    # TODO: call LinearRegressionWithGD(...) using the current alpha, to get errs and theta\n",
    "    # ...\n",
    "    errs, theta, iterations = LinearRegressionWithGD(theta_init, alpha, max_iterations, epsilon);\n",
    "    # print(\"alpha = {}, theta = {}\".format(alpha, theta))\n",
    "    print(theta)\n",
    "    # plot the errs using ax.plot(..)\n",
    "    # ...\n",
    "    ax.plot(iterations, errs)\n",
    "    \n",
    "plt.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once you have found a good $\\theta$ using gradient descent, use it to make a price prediction for a new house of 1650-square-foot with 3 bedrooms. **Note**: since the parameter vector $\\theta$ was learned using the normalized dataset, you will need to normalize the new data-point corresponding to this new house before predicting its price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293598.8223480527\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "Don't forget to normalize the feature values of this new house first.\n",
    "\"\"\"\n",
    "# Cretate a data-point x corresponding to the new house\n",
    "# Normalize the feature values of x\n",
    "# Use the vector of parameters theta to predict the price of x\n",
    "\"\"\"\n",
    "HINT: if you are not able to compute the dot product between x and theta, then \n",
    "make sure that the arrays have the same size. Did you forget something?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "datapoint = np.array([1650, 3])\n",
    "normalized_datapoint = (datapoint - mu) / std;\n",
    "\n",
    "normalized_datapoint = np.insert(normalized_datapoint, 0, 1);\n",
    "print(theta.T@normalized_datapoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Linear Regression using Tensorflow\n",
    "\n",
    "To get used to Tensorflow, let's now try to optimize the previous cost function using Tensorflow. To do this, you just need to re-implement (in the code below) the cost function $E(..)$ using Tensorflow. The following functions can be helpful:\n",
    "- Use `tf.matmul(A, B)` to compute the product of two matrices A and B. Here A and B should have the same number of dimensions and `A.shape[1]` should be the same as `B.shape[0]`.\n",
    "- Use `tf.transpose(C)` to get the transpose of a matrix C.\n",
    "\n",
    "After executing this code, if your implementation is correct, then you should obtain similar parameter values as the ones you obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TobiasPi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final parameters vector theta is: \n",
      " [[3.40403618e+05]\n",
      " [1.52561351e+05]\n",
      " [5.20632842e+01]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Define the cost function E using Tensorflow functions\n",
    "\"\"\"\n",
    "def E(theta, X, y):\n",
    "    expressionOne = tf.transpose(tf.matmul(X, theta) - y)\n",
    "    expressionTwo = tf.matmul(X, theta) - y;\n",
    "    return (1/(2*n)) * tf.matmul(expressionOne, expressionTwo)\n",
    "\n",
    "\n",
    "\"\"\" Read the following code (you don't need to modify it) :\n",
    "It minimizes the cost function E(..) that you implemented above.\n",
    "\"\"\"\n",
    "y_vec = y.reshape(-1, 1) # As a column vector\n",
    "theta = tf.Variable(np.zeros((3, 1)), dtype=tf.float64) # As a column vector (initialized to zeros)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.1) # GD with learning rate alpha set to 0.1\n",
    "for itr in range(100):\n",
    "    cost_function = lambda: E(theta, X_normalized_new, y_vec) # Our cost function\n",
    "    optimizer.minimize(cost_function, var_list=[theta]) # Apply a GD step\n",
    "    \n",
    "print(\"The final parameters vector theta is: \\n\", theta.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Normal Equation: Linear regression without gradient descent\n",
    "\n",
    "As you know from the lecture, the MSE cost function $E(\\theta)$ that we are trying to minimize is a convex function, and its derivative at the optimal $\\theta$ (that minimizes $E(\\theta)$) is equal to $0$. Therefore, to find the optimal $\\theta$, one can simply compute the derivative of $E(\\theta)$ with respect to $\\theta$, set it equal to $0$, and solve for $\\theta$.\n",
    "\n",
    "We have seen in the lecture that, by doing this, the closed-form solution is given as follows:\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"*loop until convergence*\" like in gradient descent.\n",
    "\n",
    "You are asked to implement this equation to directly compute the best parameter vector $\\theta$ for the linear regression. In Python, you can use the `inv` function from `numpy.linalg.inv` to compute the inverse of a function.\n",
    "\n",
    "Remember that while you don't need to scale your features, we still need to add a column of 1's to the $X$ matrix to have an intercept term ($\\theta_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the original (non-normalized) dataset: theta = [89597.9095428    139.21067402 -8738.01911233]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "\"\"\" TODO: \n",
    "Use the function add_all_ones_column(..) to add a column of 1's to X. \n",
    "Let's call the returned dataset X_new.\n",
    "\"\"\"\n",
    "new_X = add_all_ones_column(X);\n",
    "\n",
    "\"\"\" TODO: \n",
    "Compute the optimal theta using new_X and y (without using gradient descent).\n",
    "Use the normal equation shown above. You can use the function inv (imported above)\n",
    "to compute the inverse of a matrix.\n",
    "\"\"\"\n",
    "theta = inv((new_X.T@new_X))@new_X.T@y\n",
    "print(\"With the original (non-normalized) dataset: theta = {}\".format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once you have computed the optimal $\\theta$, use it to make a price prediction for the new house of 1650-square-foot with 3 bedrooms. Remeber that $\\theta$ was computed above based on the original dataset (without normalization); so, you do not need to normalize the feature values of the new house to make the prediction in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293081.46433489653\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "\"\"\"\n",
    "x = np.array([1, 1650, 3])\n",
    "prediction = theta.T@x\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous formula does not require any feature normalization or scaling. However, you can still compute again the optimal $\\theta$ when using `X_normalized_new` instead of `new_X`.\n",
    "\n",
    "By doing this, you will be able to compare the $\\theta$ that you compute here with the one you got previously when you used gradient descent. The two parameter vectors should be quite similar (but not necessarily exatly the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the normalized dataset: theta = [  340412.65957447   159125.15353818 -9988017.38933272]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Compute the optimal theta using X_normalized_new and y (without using gradient descent). \n",
    "Use the normal equation (shown previously).\n",
    "\"\"\"\n",
    "theta = inv((X_normalized_new.T@X_normalized_new))@X_normalized_new.T@y\n",
    "print(\"With the normalized dataset: theta = {}\".format(theta))\n",
    "# [3.40403618e+05 1.52561351e+05 5.20632846e+01]\n",
    "# Last value with the inverse function is completely off, even though the prediction in the last cell was correct. What am i missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, now that you have computed the optimal $\\theta$ based on `X_normalized_new`, use it to make a price prediction for the new house of 1650-square-foot with 3 bedrooms. Do you need to normalize the feature values of the new house here? Remeber that $\\theta$ was computed here based on the normalized dataset.\n",
    "\n",
    "You should find that this predicted price similar to the price you predicted previsouly for the same house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293081.4643348961\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "\"\"\"\n",
    "# Cretate a data-point x corresponding to the new house\n",
    "# Normalize the feature values of x\n",
    "# Use the vector of parameters theta to predict the price of x\n",
    "housefeatures = np.array([1650, 3])\n",
    "normalized_housefeatures = (housefeatures - mu) / std;\n",
    "\n",
    "normalized_housefeatures = np.insert(normalized_housefeatures, 0, 1);\n",
    "\n",
    "print(theta.T@normalized_housefeatures)\n",
    "# print(\"prediction:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Regression with scikit-learn (sklearn)\n",
    "You will now use the scikit-learn library to do the ordinary linear regression.\n",
    "\n",
    "First, the code below shows you how to scale you data using scikit-learn. The `preprocessing` module provides a class `StandardScaler` that compute the mean and standard deviation on a training data so as to be able to later reapply the same transformation on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X:\n",
      " [[2.104e+03 3.000e+00]\n",
      " [1.600e+03 3.000e+00]\n",
      " [2.400e+03 3.000e+00]\n",
      " [1.416e+03 2.000e+00]\n",
      " [3.000e+03 4.000e+00]]\n",
      "\n",
      "X_normalized:\n",
      " [[ 0.13141542 -0.22609337]\n",
      " [-0.5096407  -0.22609337]\n",
      " [ 0.5079087  -0.22609337]\n",
      " [-0.74367706 -1.5543919 ]\n",
      " [ 1.27107075  1.10220517]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit the StandardScaler to the training data (to compute the mean and std-deviation)\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "# You can now use scaler to transform (scale) the training data or any new test data\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "print(\"Original X:\\n\", X[:5])\n",
    "print()\n",
    "print(\"X_normalized:\\n\", X_normalized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to train an ordinary linear regression on the scaled training dataset. Then, use the trained model to predict the price of new test houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\"\"\" TODO:\n",
    "Train the linear regression model on the scaled training data\n",
    "\"\"\"\n",
    "reg = LinearRegression().fit(X_normalized, y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1650    3]\n",
      " [1020    2]\n",
      " [2300    4]]\n",
      "[293081.4643349  214116.75881612 374830.38333402]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use the trained regression model to predict the price of the following test houses\n",
    "* 1650-square-foot house with 3 bedrooms\n",
    "* 1020-square-foot house with 2 bedrooms\n",
    "* 2300-square-foot house with 4 bedrooms\n",
    "\"\"\"\n",
    "X_test = np.array([[1650,3], [1020, 2], [2300, 4]]); # Create the test dataset\n",
    "print(X_test)\n",
    "normalizedX = scaler.transform(X_test)\n",
    "# Scale the test dataset using scaler.transform(...)\n",
    "# Predict the prices using reg.predict(...)\n",
    "print(reg.predict(normalizedX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction you get for the first test house (`[1650, 3]`) should be similar to the prediction you got for this house when you implemented the linear regression from scrach.\n",
    "\n",
    "- For more information about data scaling in scikit-learn, check the link: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- For more information about the ordinary linear regression, check the link: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
