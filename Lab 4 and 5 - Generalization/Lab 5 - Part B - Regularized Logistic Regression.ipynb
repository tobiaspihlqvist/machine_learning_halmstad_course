{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 (Part B): Regularized Logistic Regression\n",
    "\n",
    "In this part of the Lab, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance. During quality assurance, each microchip goes through various tests to ensure it is functioning correctly. Make sure that you check the videos of lecture 5 before starting this Lab:\n",
    "- Overfitting, Generalization, and Regularization: https://youtu.be/-zVe8upJMDU\n",
    "\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "## Loading the data\n",
    "This dataset is the same as the one you used previously in the Lab about kNN classification. We have a file `microchips-dataset.csv` which contains the dataset for our classification problem. The first column corresponds to the result of *\"microchip test 1\"*, the second column corresponds to the result of *\"microchip test 2\"*, and the third column is the class-label indicating if the microchip has been accepted or rejected (1 = Accepted, 0 = Rejected).\n",
    "\n",
    "<img src=\"imgs/MicroshipDataLab3B.png\" />\n",
    "\n",
    "The following Python code loads the dataset from the csv file into the variables $X$ (input data matrix) and $y$ (output class-labels). The variable $X$ should be a matrix (as a numpy array) with $n$ lines and two columns (two feature) corresponding to *\"microchip test 1\"* and *\"microchip test 2\"*. The variable $y$ should be a numpy array of $n$ elements. Read the code and run it.\n",
    "\n",
    "A visualisation of this dataset looks like follows:\n",
    "<img src=\"imgs/MicroshipScatterPlotLab3B.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/microchips-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\") #############################\n",
    "n = len(mydata) # we have n data-points\n",
    "X = mydata[:, :2] # input data matrix (with n lines and 2 columns)\n",
    "y = mydata[:, -1] # vector of n outputs\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can here visualize the dataset as in the previous figure.\n",
    "\"\"\"\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature mapping\n",
    "From the previous visualization, you can see that our dataset cannot be separated into positive (class 1) and negative (class 0) examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary. As you might know, one way to fit the data better is to create more features from each data-point by mapping the features into polynomial terms of $x_1$ and $x_2$ (e.g. $x_1^2$, $x_1 x_2$, etc ...).\n",
    "\n",
    "The following provided function `mapFeature(..)` maps the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power. As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector (including an additional first $1$, so you don't need to add it). So, for a 2-dimensional data-point $x \\in \\mathbb{R}^2$ from the dataset $X$, it's mapped version in the new dataset `X_poly` is as follows:\n",
    "$$\n",
    "\\text{mapped version of $x$ is }\n",
    "\\begin{bmatrix}\n",
    "1\\\\ \n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\ \n",
    "x_2^2\\\\ \n",
    "x_1^3\\\\ \n",
    "\\vdots\\\\ \n",
    "x_1 x_2^5\\\\ \n",
    "x_2^6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A logistic regression classifier trained on this higher-dimension feature-vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot. While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the Lab, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature mapping function to polynomial features. This function Maps a dataset of two input \n",
    "# features X1, X2 (which must be of the same size) to quadratic features. Returns a new feature \n",
    "# array with more features, comprising of X1, X2, (X1**2), (X2**2), X1*X2, X1*(X2**2), etc..\n",
    "def mapFeature(X1, X2):\n",
    "    degree = 6\n",
    "    col = 0\n",
    "    out = np.ones((len(X1), 27))\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(0, i+1):\n",
    "            out[:, col] = (X1**(i-j)) * (X2**j)\n",
    "            col += 1\n",
    "    \n",
    "    out = np.c_[ np.ones(len(out)), out ] # add a first column of ones\n",
    "    return out\n",
    "\n",
    "# We call mapFeature on the original dataset X (with two features) to get a mapped \n",
    "# dataset X_poly with 28 features (which already includes a first column of ones).\n",
    "X_poly = mapFeature(X[:, 0], X[:, 1])\n",
    "print(\"Shape of X_poly is:\", X_poly.shape)\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can print here a small subset of X_poly to see how to looks like\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and gradient\n",
    "Your task here is to complete the following Python code to compute the cost function and gradient for regularized logistic regression. Recall that the regularized cost function in logistic regression is as follows (note that you should not regularize the parameter $\\theta_0$).\n",
    "$$E(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left [ -y^{(i)} \\log{(h_\\theta(x^{(i)}))} - (1 - y^{(i)}) \\log{(1 - h_\\theta(x^{(i)}))} \\right ] + \\frac{\\lambda}{2n} \\sum_{j=1}^{d} \\theta_j^2$$\n",
    "\n",
    "The gradient of the cost function is a vector where the $j^{th}$ element in this vector is defined as follows:\n",
    "$$\n",
    "\\frac{\\partial E(\\theta)}{\\partial \\theta_0} = \\frac{1}{n} \\sum_{i=1}^{n} ( h_\\theta(x^{(i)}) - y^{(i)} ) ~ x_j^{(i)}\n",
    "\\quad \\quad \\quad \\quad \\quad \\quad \\text{for } j = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\theta)}{\\partial \\theta_j} = \\left ( \\frac{1}{n} \\sum_{i=1}^{n} ( h_\\theta(x^{(i)}) - y^{(i)} ) ~ x_j^{(i)} \\right ) + \\frac{\\lambda}{n} \\theta_j\n",
    "\\quad \\quad \\text{for } j \\geq 1\n",
    "$$\n",
    "\n",
    "Once you are done, call your cost function $E(\\theta)$ to test if it's working correctly using the initial value of $\\theta$ (initialized to all zeros). You should see that the cost is about $0.693$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sigmoid function works on a scalar value as well as a vector or a matrix\n",
    "def sigmoid(z):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write the definition of the cost function E(..) for regularized logistic regression. \n",
    "\"\"\"\n",
    "def E(theta, X, y, lmd):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write the definition of the corresponding gradient function gradE(..)\n",
    "\"\"\"\n",
    "def gradE(theta, X, y, lmd):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call your cost function and gradient using the initial value of theta (initialized to all zeros) and lmd = 0\n",
    "\"\"\"\n",
    "theta_init = np.zeros(X_poly.shape[1])\n",
    "# print(\"Cost: \", E(...))\n",
    "# print(\"Gradient vector: \", gradE(...))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning parameters using scipy.optimize.minimize(..)\n",
    "Once you have completed the cost and gradient for regularized logistic regression correctly, you will use `scipy.optimize.minimize(..)` to learn the optimal parameters $\\theta$. To do this, complete the definition of the function `trainLogisticReg(..)` in the following Python code. The function should return the optimal parameters vector $\\theta$ that it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function trainLogisticReg(...). You can use op.minimize(..) as in previous Labs.\n",
    "\"\"\"\n",
    "def trainLogisticReg(X, y, theta_init, lmd):\n",
    "    # TODO: Use op.minimize(..) to minimize the cost function E, \n",
    "    # the optimal parameters can then be accessed with: theta = res.x\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call the function trainLogisticReg(..) to check if it is working correctly. The final cost computed \n",
    "based on the optimal theta, should be smaller than the initial cost computed based on theta_init.\n",
    "\"\"\"\n",
    "theta_init = np.zeros(X_poly.shape[1])  # Some initial theta (of same dimension as the number of features)\n",
    "lmd = 0  # The regularization parameter\n",
    "\n",
    "print(\"Initial cost: \", E(theta_init, X_poly, y, lmd))\n",
    "# theta = trainLogisticReg(X_poly, y, theta_init, lmd)\n",
    "# print(\"Final cost: \", E(theta, X_poly, y, lmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision boundary\n",
    "To help you visualize the model learned by this classifier, we have provided the function `trainAndPlotDecisionBoundary(..)` which trains a regularized logistic regression by calling the previously defined function `trainLogisticReg(..)`, then plots the (nonlinear) decision boundary that separates the positive and negative examples. In `trainAndPlotDecisionBoundary(..)`, we plot the nonlinear decision boundary by computing the classifier's predictions on an evenly spaced grid and then and drew a contour plot of where the predictions change from $y = 0$ to $y = 1$. You can read the code of this function before running it, but you don't have to fully understand all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainAndPlotDecisionBoundary(X_original, X_poly, y, theta_init, lmd):\n",
    "    # Training\n",
    "    theta = trainLogisticReg(X_poly, y, theta_init, lmd)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"Microship Test 1\")\n",
    "    ax.set_ylabel(\"Microship Test 2\")\n",
    "    \n",
    "    # Plotting the original dataset\n",
    "    X0 = X_original[y==0]\n",
    "    X1 = X_original[y==1]\n",
    "    ax.scatter(X0[:, 0], X0[:, 1], color=\"red\", marker=\"x\", label=\"Rejected (y=0)\")\n",
    "    ax.scatter(X1[:, 0], X1[:, 1], color=\"blue\", marker=\"+\", label=\"Accepted (y=1)\")\n",
    "    \n",
    "    # Plotting the nonlinear decision boundary\n",
    "    x1_plot = np.linspace(-1, 1.5, 50)\n",
    "    x2_plot = np.linspace(-1, 1.5, 50)\n",
    "    X1_plot, X2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    \n",
    "    z = np.zeros((len(x1_plot), len(x2_plot)))\n",
    "    for i in range(len(x1_plot)):\n",
    "        for j in range(len(x2_plot)):\n",
    "            z[i][j] = mapFeature(x1_plot[i:i+1], x2_plot[j:j+1]) @ theta\n",
    "            \n",
    "    ax.contour(X1_plot, X2_plot, z, [0], colors=\"green\")\n",
    "    \n",
    "    ax.set_title(\"Dataset and decision boundary plot with lambda = {}\".format(lmd))\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Here we call the function using the an initial theta of zeros, and lmd = 0 (no regularization):\n",
    "trainAndPlotDecisionBoundary(X, X_poly, y, theta_init, lmd=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different values for the regularization parameter $\\lambda$\n",
    "In this section, you will get to try out different regularization parameters for the dataset to understand how regularization prevents overfitting. In the Python code below, you can call `trainAndPlotDecisionBoundary(..)` using $\\lambda = 0$, $\\lambda = 0.01$, $\\lambda = 0.5$, $\\lambda = 1$, $\\lambda = 10$, and $\\lambda = 100$ (you can try other values as well if you want).\n",
    "\n",
    "Notice the changes in the decision boundary as you vary $\\lambda$. With a small $\\lambda$, you should find that the classifier gets most training examples correct, but draws a complicated boundary, thus overfitting the data. This is not a good decision boundary. With a larger $\\lambda$, you should see a plot that shows a simpler decision boundary which still separates the positives and negatives fairly well. However, if $\\lambda$ is set to a too high value, you will not get a good fit and the decision boundary will not follow the data so well, thus underfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "Call the function trainAndPlotDecisionBoundary(..) with various values of lmd\n",
    "\"\"\"\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
